import torch
import torchvision
from torch.utils.data import DataLoader
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.datasets import CocoDetection
from torchvision.transforms import functional as F
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm # Import tqdm for the progress bar

# Define transformations
class CocoTransform:
    def __call__(self, image, target):
        image = F.to_tensor(image)  # Convert PIL image to tensor
        return image, target

# Dataset class
def get_coco_dataset(img_dir, ann_file):
    return CocoDetection(
        root=img_dir,
        annFile=ann_file,
        transforms=CocoTransform()
    )

# Load datasets
test_dataset = get_coco_dataset(
    img_dir="/content/violence-detection-4/test",
    ann_file="/content/violence-detection-4/test/_annotations.coco.json"
)

val_dataset = get_coco_dataset(
    img_dir="/content/violence-detection-4/valid",
    ann_file="/content/violence-detection-4/valid/_annotations.coco.json"
)

# --- Add this section to check for empty ground truth images ---
empty_gt_images_in_val = 0
for i in range(len(val_dataset)):
    try:
        # CocoDetection's __getitem__ returns (image, annotations_list)
        # annotations_list is a list of dictionaries, one per object
        # If no objects, it's an empty list.
        _, annotations = val_dataset.coco.loadAnns(val_dataset.coco.getAnnIds(imgIds=val_dataset.ids[i]))
        if not annotations: # Check if the list of annotations is empty
            empty_gt_images_in_val += 1
            # You can also print the image ID if you want to know which ones
            # print(f"Image ID {val_dataset.ids[i]} has no ground truth objects.")
    except Exception as e:
        print(f"Error checking val_dataset item {i}: {e}")
        # This can happen if an image file is missing, etc.
        continue

print(f"\nNumber of images in the validation dataset with NO ground truth objects: {empty_gt_images_in_val}")
# --- End of added section ---


# DataLoader
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

# Load Faster R-CNN with ResNet-50 backbone
def get_model(num_classes):
    # Load pre-trained Faster R-CNN
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
    # Get the number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # Replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

# Initialize the model
num_classes = 3  # violence-no_violence, non_violence, violence

# Move model to GPU if available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
# Load the trained model
model = get_model(num_classes)
model.load_state_dict(torch.load("fasterrcnn_resnet50_epoch_5.pth"))
model.to(device)
model.eval()  # Set the model to evaluation mode

def prepare_image(image_path):
    image = Image.open(image_path).convert("RGB")  # Open image
    image_tensor = F.to_tensor(image).unsqueeze(0)  # Convert image to tensor and add batch dimension
    return image_tensor.to(device)

# Load the unseen image
image_path = "/content/violence-detection-4/test/0x0_webp.rf.e08e73c05bcec1144b99ff6db9c8d8fb.jpg"
image_tensor = prepare_image(image_path)

with torch.no_grad():  # Disable gradient computation for inference
    prediction = model(image_tensor)

# `prediction` contains:
# - boxes: predicted bounding boxes
# - labels: predicted class labels
# - scores: predicted scores for each box (confidence level)
COCO_CLASSES = {0: "violence-no_violence", 1: "non_violence", 2: "violence"}

def get_class_name(class_id):
    return COCO_CLASSES.get(class_id, "Unknown")

# Draw bounding boxes with the correct class names and increase image size
def draw_boxes(image, prediction, fig_size=(10, 10)):
    boxes = prediction[0]['boxes'].cpu().numpy()  # Get predicted bounding boxes
    labels = prediction[0]['labels'].cpu().numpy()  # Get predicted labels
    scores = prediction[0]['scores'].cpu().numpy()  # Get predicted scores

    # Set a threshold for showing boxes (e.g., score > 0.5)
    threshold = 0.5

    # Set up the figure size to control the image size
    plt.figure(figsize=fig_size)  # Adjust the figure size here
    plt.imshow(image)  # Display the image once

    for box, label, score in zip(boxes, labels, scores):
        if score > threshold:
            x_min, y_min, x_max, y_max = box
            class_name = get_class_name(label)  # Get the class name
            plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,
                                              linewidth=2, edgecolor='r', facecolor='none'))
            plt.text(x_min, y_min, f"{class_name} ({score:.2f})", color='r')

    plt.axis('off')  # Turn off axis
    plt.show()

# Display the image with bounding boxes and correct labels
draw_boxes(Image.open(image_path), prediction, fig_size=(12, 10))  # Example of increased size

def calculate_detection_metrics(model, data_loader, device, iou_threshold=0.5, score_threshold=0.5):
    model.eval()
    tp = 0
    fp = 0
    fn = 0
    tn = 0 # Initialize True Negatives
    total_images_processed = 0 # Track total images processed

    with torch.no_grad():
        for images, targets in tqdm(data_loader, desc="Calculating Detection Metrics"): # Added tqdm for progress
            total_images_processed += len(images) # Accumulate total images processed
            images = [img.to(device) for img in images]

            processed_targets = []
            for target_list_for_image in targets: # targets is a tuple of lists of dicts
                boxes = []
                labels = []
                for obj in target_list_for_image:
                    bbox = obj["bbox"]
                    x, y, w, h = bbox
                    if w > 0 and h > 0:
                        boxes.append([x, y, x + w, y + h])
                        labels.append(obj["category_id"])
                if boxes:
                    processed_targets.append({
                        "boxes": torch.tensor(boxes, dtype=torch.float32),
                        "labels": torch.tensor(labels, dtype=torch.int64),
                    })
                else:
                    processed_targets.append({"boxes": torch.empty((0, 4), dtype=torch.float32), "labels": torch.empty((0,), dtype=torch.int64)}) # Handle images with no objects

            predictions = model(images)

            for i in range(len(images)):
                pred_boxes = predictions[i]['boxes'].cpu()
                pred_labels = predictions[i]['labels'].cpu()
                pred_scores = predictions[i]['scores'].cpu()

                gt_boxes = processed_targets[i]['boxes'].cpu()
                gt_labels = processed_targets[i]['labels'].cpu()

                # Filter predictions by a confidence threshold
                confident_indices = pred_scores > score_threshold
                pred_boxes_filtered = pred_boxes[confident_indices]
                pred_labels_filtered = pred_labels[confident_indices]

                # Calculate TP, FP, FN for the current image
                image_tp, image_fp, image_fn = calculate_per_image_metrics(
                    pred_boxes_filtered, pred_labels_filtered, gt_boxes, gt_labels, iou_threshold
                )
                tp += image_tp
                fp += image_fp
                fn += image_fn

                # Calculate TN for the current image (image-level)
                # An image is a True Negative if it has no ground truth objects AND
                # the model predicts no objects (after thresholding).
                if len(gt_boxes) == 0 and len(pred_boxes_filtered) == 0:
                    tn += 1

    return tp, fp, fn, tn, total_images_processed # Return total_images_processed as well

def calculate_per_image_metrics(pred_boxes, pred_labels, gt_boxes, gt_labels, iou_threshold):
    tp = 0
    fp = 0
    fn = 0

    matched_pred_indices = set()
    matched_gt_indices = set()

    for gt_idx, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):
        best_iou = 0
        best_pred_idx = -1

        for pred_idx, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):
            if pred_idx in matched_pred_indices:
                continue # Skip already matched predictions

            iou = calculate_iou(pred_box, gt_box)

            if iou > best_iou and pred_label == gt_label:
                best_iou = iou
                best_pred_idx = pred_idx

        if best_iou >= iou_threshold and best_pred_idx != -1:
            tp += 1
            matched_pred_indices.add(best_pred_idx)
            matched_gt_indices.add(gt_idx)

    fp = len(pred_boxes) - len(matched_pred_indices)
    fn = len(gt_boxes) - len(matched_gt_indices)

    return tp, fp, fn

def calculate_iou(box1, box2):
    x_min1, y_min1, x_max1, y_max1 = box1
    x_min2, y_min2, x_max2, y_max2 = box2

    inter_x_min = max(x_min1, x_min2)
    inter_y_min = max(y_min1, y_min2)
    inter_x_max = min(x_max1, x_max2)
    inter_y_max = min(y_max1, y_max2)

    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)

    area1 = (x_max1 - x_min1) * (y_max1 - y_min1)
    area2 = (x_max2 - x_min2) * (y_max2 - y_min2)

    union_area = area1 + area2 - inter_area

    if union_area == 0:
        return 0
    return inter_area / union_area

# Call the function to get the raw counts and total images
tp, fp, fn, tn, total_images = calculate_detection_metrics(model, val_loader, device, iou_threshold=0.5)

# Print TP, FP, FN, TN using the content moderation terminology
print(f"\n--- Confusion Matrix Counts (Content Moderation System) ---")
print(f"True Positives (Correctly detected harmful content): {tp}")
print(f"False Positives (Safe content incorrectly flagged as harmful): {fp}")
print(f"False Negatives (Harmful content missed by the system): {fn}")
print(f"True Negatives (Correctly identified safe content): {tn}")
print(f"Total Images Processed: {total_images}")


# --- Compute and Print Metrics based on your provided formulas ---

# Precision
# Formula: TP / (TP + FP)
precision_denominator = tp + fp
precision = tp / precision_denominator if precision_denominator > 0 else 0
print(f"\nPrecision = TP / (TP + FP)")
print(f"Precision = {tp} / ({tp} + {fp})")
print(f"Precision = {tp} / {precision_denominator}")
print(f"Precision: {precision:.4f}")

# Recall
# Formula: TP / (TP + FN)
recall_denominator = tp + fn
recall = tp / recall_denominator if recall_denominator > 0 else 0
print(f"\nRecall = TP / (TP + FN)")
print(f"Recall = {tp} / ({tp} + {fn})")
print(f"Recall = {tp} / {recall_denominator}")
print(f"Recall: {recall:.4f}")

# F1 Score
# Formula: 2 * (Precision * Recall) / (Precision + Recall)
f1_denominator = precision + recall
f1_score = (2 * precision * recall) / f1_denominator if f1_denominator > 0 else 0
print(f"\nF1 Score = 2 * (Precision * Recall) / (Precision + Recall)")
print(f"F1 Score = 2 * ({precision:.4f} * {recall:.4f}) / ({precision:.4f} + {recall:.4f})")
print(f"F1 Score = {2 * precision * recall:.4f} / {f1_denominator:.4f}")
print(f"F1 Score: {f1_score:.4f}")

# Accuracy
# Formula: (TP + TN) / (TP + TN + FP + FN)
accuracy_numerator = tp + tn
accuracy_denominator = tp + tn + fp + fn
accuracy = accuracy_numerator / accuracy_denominator if accuracy_denominator > 0 else 0
print(f"\nAccuracy = (TP + TN) / (TP + TN + FP + FN)")
print(f"Accuracy = ({tp} + {tn}) / ({tp} + {tn} + {fp} + {fn})")
print(f"Accuracy = {accuracy_numerator} / {accuracy_denominator}")
print(f"Accuracy: {accuracy:.4f}")

# Average Precision (AP) and Mean Average Precision (mAP)
print("\nNote: Mean Average Precision (mAP) and Average Precision (AP) are not directly")
print("computable from simple sums of TP, TN, FP, FN. They require more complex")
print("Precision-Recall curve analysis (e.g., using libraries like torchmetrics).")
